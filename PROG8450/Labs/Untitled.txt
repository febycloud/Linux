/*********************************************************
*********************************************************
	Linear Regression Example in Spark 1.6.x
*********************************************************
*********************************************************


-- We will be working with a CSV file
-- Spark 1.6 does not have built-in way of dealing with this
-- so we will manually add jar files (classes essentially) to do the work for us
*/

spark-shell --master yarn --jars /shared/commons-csv-1.5.jar,/shared/spark-csv_2.10-1.5.0.jar

/*
-- NOTE: The above command assumes that the jar files are in the same location as where you run the spark command
-- Otherwise, you have to give the full path

-- Just a bunch of import statements, you can run these in a paste command
*/

import org.apache.spark.sql.functions._
import org.apache.spark.ml.feature.{VectorAssembler}
import org.apache.spark.ml.Pipeline
import org.apache.spark.ml.regression.{LinearRegression}
import org.apache.spark.ml.tuning.{CrossValidator, CrossValidatorModel, ParamGridBuilder}
import org.apache.spark.ml.evaluation.{RegressionEvaluator}
import org.apache.spark.ml.param.ParamMap
import org.apache.spark.sql.types.{DoubleType}

/*
-- SQLcontext to deal with CSV files in Spark 1.6 and lower
-- If you ever end up working in Spark 2.0 and above, the commands to load a CSV will be slightly different
*/

import org.apache.spark.sql.SQLContext
val sqlContext = new SQLContext(sc)

/*
-- Loading our CSV file, note the inferSchema option being set to true
*/

val data_2015 = sqlContext.read.format("com.databricks.spark.csv")
.option("header", "true") 
.option("inferSchema", "true")
 .load("hdfs://localhost:8020/BigData/happiness_data.csv")

/*
-- Here we are just selecting two columns from our dataset --> The happiness rank and happiness score
*/

val rank_score = data_2015.select(col("Happiness Rank").cast(DoubleType), col("Happiness Score").cast(DoubleType),col("Social support").cast(DoubleType),col("Generosity").cast(DoubleType))

/*
-- Next we split the dataset into training and test sets
-- We use a randomSplit function with a split percentage of 80%, and 20%
-- The second argument to the function is the seed, it is used to get the same random results every time
*/

val Array(trainingData, testData) = rank_score.randomSplit(Array(0.8, 0.2), 1111) 

/*
### Machine Learning Steps ###

-- All the following steps will be the same for all machine learning problems

-- Preparing features and labels 

-- We need to prepare our features and labels to supply it to our algorithm, in this case linear regression
-- Remember features are the input data to our algorithm and labels are the values our algorithm is going to train the model to predict
-- Features are also called independent variables
-- Lables are also called dependent variables

-- In our problem, we only have one FEATURE, it is the happiness score
-- We will predict the happiness rank, which becomes the LABEL

-- We will pass an array to the InputCols with the name of all the features 
-- And we will simply give the name of the output column

-- Think of a vector as an Array
-- Vector assembler can be used to package one feature or multiple features into a vector
-- And this will be our feature column
-- We pass an array to inputCols with the name of all the features we want to assemple
*/

val assembler = new VectorAssembler()
.setInputCols(Array("Happiness Score","Social support","Generosity"))
.setOutputCol("assembled-features")

/*
-- Next we will instantiate our algorithm, in this case linear regression, and set the features and label column
-- In this case, the features column will be the output from VectorAssembler which is the assembled features
-- And label is the value linear regression will try to predict which is the happiness rank
-- Features column will be the output form the vector assempler
*/

val lr = new LinearRegression() 
 .setFeaturesCol("assembled-features")
 .setLabelCol("Happiness Rank")

/*
-- Next we create a pipeline for everythign that needs to be executed
-- Think of the pipeline as the assembly line in the factory where all the parts for a product are assembled together
-- This is what we are doing with pipeline
-- We create STAGES in pipelines
-- In this example, our pipeline has only two stages
-- The first stage is where we assemble the features with VectorAssembler
-- And the second stage, we'll specify the algorithm that is used to train the model
*/

val pipeline = new Pipeline()
 .setStages(Array(assembler, lr))

/*
-- Once we train the model, we need to evaluate the model for accuracy
-- Since this is a regression problem, we will use RegressionEvaluator to evaluate the model
-- The model, once it performs the predictions, it will store the prediction results in the predition column
-- Once a prediction is made, we need to evalue the prediction with the actual value, so we know how good the prediction is
-- So in our case, our model will predict the ranking based on our score
-- We need to see if the the prediction matches the actual or not
-- If it doesn't match, we want to know how far the prediction is from the actual value
-- We will use a metric called r squared to show the accuracy of the model prediction
-- r squared is also called coefficient of determination
-- r squared basically tells us how good our model is
-- The higher the r2 model the better our model
*/

val evaluator = new RegressionEvaluator()
 .setLabelCol("Happiness Rank")
 .setPredictionCol("prediction")
 .setMetricName("r2")

/*
-- Next we setup our cross validator
-- cross validator is an interesting concept
-- We need to provide two things to the CrossValidator
-- An estimator and an evaluator
-- Estimator we provide the pipeline
-- For the evaluator we provide regression evaluator 
-- We can provide additinoal parameters called "hyper parameters" which can be used for tuning our model
-- We don't have any hyper parameters so we create an instance of the parameter Grid Builder and call the build method
-- This will provide an empty parameter map
-- Finally we specify the fold as 3
-- This means the cross validator will generate three sets of data from our initial training dataset
-- In other words it "folds" the data into 3 and from each sets of data, it will use 2/3 of the data for training and 1/3 of the data for testing
-- And then it will fix the model based on the best accuracy based on the defined evaluation metric, in our case r2
*/

val cross_validator = new CrossValidator()
 .setEstimator(pipeline)
 .setEvaluator(evaluator)
 .setEstimatorParamMaps(new ParamGridBuilder().build)
 .setNumFolds(3)

/*
-- Next we call fit on the cross validator passing our trainig dataset
-- CrossValidator will now fold the dataset into 3, divide each subset into training and test set
-- Inside each fold, it will use the training set to train and test set to evaluate the model based on r2
-- The best of the three models is returned
*/

val cvModel = cross_validator.fit(trainingData)

/*
-- Now cvModel is a trained model that we can use to make predictions
-- We can actually save this model and reuse it as well
-- We'll see how to do that later

-- Before we can use our model on the real data, let's test our data on the real data to see how good it is
-- We now called cvModel.transform on the testData set
-- when we call transform, our model will go over the dataset and make predictions
*/

val predictions = cvModel.transform(testData)

/*
-- Remember, the directory SHOULD NOT EXIST, remove it first if it does
-- hadoop fs -rm -r /BigData/happiness/output/
-- Once the model has made predictions, we are just selecting the "Happiness Rank", "Happiness Socre" and the prediction column from the model
-- and write it as a CSV file in a location in HDFS
*/

predictions
 .select(col("Happiness Rank"), col("Happiness Score"), col("prediction"))
 .write
 .format("csv")
 .save("hdfs://localhost:8020/BigData/happiness/output/")

/*
-- Finally, we call the evaluator method on the evaluator passing the preditions DataFrame
-- It will give us back the r2 result
-- R2 is presented as a percentage, if we get 0.9, means our model is 90% accurate
*/

val r2 = evaluator.evaluate(predictions)

println("r-squared on test data = " + r2)